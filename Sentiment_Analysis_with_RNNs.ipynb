{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxq0ItPUDNcASZkRZAHkRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akankshakusf/Project-DeepLearning-Sentiment-Analysis-of-IMDB-Movie-Reviews/blob/master/Sentiment_Analysis_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hdYcw5AuGUVF"
      },
      "outputs": [],
      "source": [
        "# import ML packages\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import string\n",
        "import time\n",
        "import re\n",
        "import numpy as random\n",
        "import gensim.downloader as api\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "\n",
        "# import tf DL packages\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Conv1D, InputLayer,BatchNormalization, Bidirectional, Dense, Flatten,Dropout,Input,Embedding,TextVectorization\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy, TopKCategoricalAccuracy, CategoricalAccuracy,SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive, files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade numpy\n",
        "# !pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "OGQj_5EAHXGT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "AA_3k_sFIZyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds,val_ds,test_ds=tfds.load('imdb_reviews', split=['train', 'test[:50%]', 'test[50%:]'],as_supervised=True)"
      ],
      "metadata": {
        "id": "uGw9BqCfIaN3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnsHikJoIc-Y",
        "outputId": "efe8d118-e292-4968-efd3-87e7c9040d71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for review,label in val_ds.take(2):\n",
        "  print(review)\n",
        "  print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEalsUTlIfsg",
        "outputId": "d3f325eb-7fb7-44ef-9a2e-6a8fa43190b8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\", shape=(), dtype=string)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.<br /><br />The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.<br /><br />This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\", shape=(), dtype=string)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we standarize all the values\n",
        "reference : https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace\n",
        "- https://github.com/google/re2\n",
        "- convert to input lowercase\n",
        "- review html tags through regex (regular expression)\n",
        "- take off punctuations\n",
        "- remove special chracters\n",
        "- remove accented chracters\n",
        "- reduce the word to root through one of these:\n",
        "  - Stemming\n",
        "    - stemming : PorterStemmer\n",
        "    - lemmatization : Lematize\n",
        "  - Tokenization\n",
        "    - chracter tokenization\n",
        "    - word tokenization\n",
        "    - subword tokenization\n",
        "    - n-gram tokenization\n",
        "  - Vectorization\n",
        "    - one-hot\n",
        "    - bag of words\n",
        "    - tf-idf\n",
        "    - embeddings\n",
        "    "
      ],
      "metadata": {
        "id": "SHENIYslIuPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Standardization"
      ],
      "metadata": {
        "id": "hT62JE0vJp-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(input_data):\n",
        "  '''\n",
        "  Input: raw reviews\n",
        "  output: standardized reviews\n",
        "  '''\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  no_tag = tf.strings.regex_replace(lowercase,\"<[^>]+>\",\"\")\n",
        "  output = tf.strings.regex_replace(no_tag,\"[%s]\"%re.escape(string.punctuation),\"\")\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "2AOYOVLVIfp0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standardization(\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIoBluFkIfmJ",
        "outputId": "d41dc166-b1f3-4fe2-f7e6-9d23b8e51cc6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'there are films that make careers for george romero it was night of the living dead for kevin smith clerks for robert rodriguez el mariachi add to that list onur tukels absolutely amazing dingalingless flawless filmmaking and as assured and as professional as any of the aforementioned movies i havent laughed this hard since i saw the full monty and even then i dont think i laughed quite this hard so to speak tukels talent is considerable dingalingless is so chock full of double entendres that one would have to sit down with a copy of this script and do a linebyline examination of it to fully appreciate the uh breadth and width of it every shot is beautifully composed a clear sign of a surehanded director and the performances all around are solid theres none of the overthetop scenery chewing one mightve expected from a film like this dingalingless is a film whose time has come'>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define vocab size and sequence length"
      ],
      "metadata": {
        "id": "eANj6ej08wW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "SEQUENCE_LENGTH = 250"
      ],
      "metadata": {
        "id": "kCC5MtmYIfeq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- reference  : https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization"
      ],
      "metadata": {
        "id": "ukNV2d7X3kx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    standardize= standardization,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQUENCE_LENGTH\n",
        "  )\n",
        ""
      ],
      "metadata": {
        "id": "0RR4XJDb3LbU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Note : here in the above cell VOCAB_SIZE, SEQUENCE_LENGTH have been taken up with some guessing values. But we can also take a look at the data understand exactly what the values should be set to\n"
      ],
      "metadata": {
        "id": "nMD3XKFf32OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this takes while as it is going through all review of samples selected\n",
        "lengths=[] #initalized empty lists\n",
        "words=[]\n",
        "\n",
        "for review, label in train_ds.take(10):\n",
        "  for word in tf.strings.split(review, sep = \"\"):\n",
        "    if word in words:\n",
        "      pass\n",
        "    else:\n",
        "      words.append(word) # append the word to list\n",
        "  lengths.append(len(tf.strings.split(review, sep=\"\"))) # append the len to list"
      ],
      "metadata": {
        "id": "_jnIioV94NrE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words))\n",
        "print(lengths)"
      ],
      "metadata": {
        "id": "44VC5wm35xMl",
        "outputId": "2b2388de-240c-4395-f1e0-b0d00033b45b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "913\n",
            "[116, 112, 132, 88, 81, 289, 557, 111, 223, 127]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Note : after observing above result we can say that just by taking 10 samples we can see that the length is coming out to be 913. which means that lenght of whole data set will be quite big so we took 10000.\n",
        "- Also for the sequence length you can see that the lenght is more than 500. max we see in the set of 10 is [116, 112, 132, 88, 81, 289, 557, 111, 223, 127]. So, yes I think that random values we have taken are good\n",
        "- just for testing i change 10 to 100 samples for better visibility"
      ],
      "metadata": {
        "id": "ibg9U6Ns6PTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets take mean and median to understand better\n",
        "print(np.mean(lengths))\n",
        "print(np.median(lengths))"
      ],
      "metadata": {
        "id": "MWjhl3wP6LvT",
        "outputId": "134ce38f-02c2-4c35-c460-f52b54c5d028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "183.6\n",
            "121.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- reference :https://stackoverflow.com/questions/72688923/how-to-adapt-textvectorization-layer-on-tf-dataset"
      ],
      "metadata": {
        "id": "_HllZHy09cBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets attach this Vectorizer to dataset to get the vocabulary list\n",
        "training_dataset = train_ds.map(lambda x, y :x)  ## input x, y and output\n",
        "vectorize_layer.adapt(training_dataset)  ##adapt the vectorizer layer to the training dataset"
      ],
      "metadata": {
        "id": "bBfuuz-c868X"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Get Vocabulary out from the vectorizer"
      ],
      "metadata": {
        "id": "-zEd5f8y-amP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectorize_layer.get_vocabulary())"
      ],
      "metadata": {
        "id": "PPJaaSk9-XBU",
        "outputId": "c6aa8e8c-b164-456d-d719-a143bd04778e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for review,label in train_ds.take(1):\n",
        "  print(review)"
      ],
      "metadata": {
        "id": "MPWdPrX3865y",
        "outputId": "9ff54f3a-dbdc-41f3-bd89-ddadbbec8494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the numerical vectors out by passing into vectorize_layer\n",
        "def vectorizer(review, label):\n",
        "  return vectorize_layer(review), label"
      ],
      "metadata": {
        "id": "lIBVc8G2863F"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_ds.map(vectorizer)\n",
        "val_dataset = val_ds.map(vectorizer)"
      ],
      "metadata": {
        "id": "5zK2fbch860U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review vectorized data\n",
        "for review,label in train_dataset.take(1):\n",
        "  print(review)\n",
        "  print(label)"
      ],
      "metadata": {
        "id": "3s8ERpniBA3r",
        "outputId": "a546a153-71b3-4504-c9ff-504c81bff3cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  10   13   33  411  384   17   89   26    1    8   32 1337 3521   40\n",
            "  491    1  192   22   84  149   18   10  215  317   26   64  239  212\n",
            "    8  484   54   64   84  111   95   21 5502   10   91  637  737   10\n",
            "   17    7   33  393 9554  169 2443  406    2   87 1205  135   65  142\n",
            "   52    2    1 7408   65  245   64 2832   16    1 2851    1    1 1415\n",
            " 4969    3   39    1 1567   15 3521   13  156   18    4 1205  881 7874\n",
            "    8    4   17   12   13 4037    5   98  145 1234   11  236  696   12\n",
            "   48   22   91   37   10 7285  149   37 1337    1   49  396   11   95\n",
            " 1148  841  140    9    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(250,), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- prefetch the data so we have optimal perfromance and parallelism"
      ],
      "metadata": {
        "id": "QyasuicmCUmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prefetch the data so we have optimal perfromance and parallelism\n",
        "train_dataset = train_dataset.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size = tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "cxwVOVHMCAGc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I9Vt4agQC10E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}